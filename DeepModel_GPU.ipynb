{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive to Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split folders with images into train and test folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The provided input folder \"\\PlantVillage-Dataset\\ raw\\color\" does not exists. Your relative path cannot be found from the current working directory \"c:\\Project\\Leaf_Assist\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pre_data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mPlantVillage-Dataset\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m raw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcolor\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      4\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/PlantVillage-Dataset/raw/color_split/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m splitfolders\u001b[39m.\u001b[39;49mratio(pre_data_dir, output\u001b[39m=\u001b[39;49mdata_dir, seed\u001b[39m=\u001b[39;49m\u001b[39m1337\u001b[39;49m, ratio\u001b[39m=\u001b[39;49m(\u001b[39m.1\u001b[39;49m, \u001b[39m.9\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\VIDYADHARY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\splitfolders\\split.py:81\u001b[0m, in \u001b[0;36mratio\u001b[1;34m(input, output, seed, ratio, group_prefix, move)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(ratio) \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`ratio` should\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m check_input_format(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m use_tqdm:\n\u001b[0;32m     84\u001b[0m     prog_bar \u001b[39m=\u001b[39m tqdm(desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCopying files\u001b[39m\u001b[39m\"\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m files\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\VIDYADHARY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\splitfolders\\split.py:56\u001b[0m, in \u001b[0;36mcheck_input_format\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m p_input\u001b[39m.\u001b[39mis_absolute():\n\u001b[0;32m     55\u001b[0m         err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Your relative path cannot be found from the current working directory \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mPath\u001b[39m.\u001b[39mcwd()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg)\n\u001b[0;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m p_input\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m     59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe provided input folder \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is not a directory\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The provided input folder \"\\PlantVillage-Dataset\\ raw\\color\" does not exists. Your relative path cannot be found from the current working directory \"c:\\Project\\Leaf_Assist\"."
     ]
    }
   ],
   "source": [
    "# !pip install spilt_folders\n",
    "import splitfolders\n",
    "pre_data_dir = '\\PlantVillage-Dataset\\raw\\color'\n",
    "data_dir = '/PlantVillage-Dataset/raw/color_split/'\n",
    "splitfolders.ratio(pre_data_dir, output=data_dir, seed=1337, ratio=(.1, .9))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a Deep Learning model using a pretrained model \n",
    "\n",
    "We choose **AlexNet** model and use a Feature Extraction Transfer Learning to train it and build our model. We will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Specify the input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format of the data directory conforms to the ImageFolder structure\n",
    "data_dir = '/PlantVillage-Dataset/raw/color_split/'\n",
    "# data_dir = os.getcwd()+'/PlantVillage/raw/color_split/'\n",
    "# A CNN model to choose \n",
    "model_name = 'alexnet'\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 38\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 134\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 30\n",
    "\n",
    "# Flag for feature extracting. When True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_device_name(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Training: Data aumentation and normalization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Validation: Just normalization\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data_transforms \u001b[39m=\u001b[39m {\n\u001b[1;32m----> 4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      5\u001b[0m         transforms\u001b[39m.\u001b[39mRandomResizedCrop(\u001b[39m224\u001b[39m),\n\u001b[0;32m      6\u001b[0m         transforms\u001b[39m.\u001b[39mRandomHorizontalFlip(),\n\u001b[0;32m      7\u001b[0m         transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m      8\u001b[0m         transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m      9\u001b[0m     ]),\n\u001b[0;32m     10\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m: transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     11\u001b[0m         transforms\u001b[39m.\u001b[39mResize(\u001b[39m256\u001b[39m),\n\u001b[0;32m     12\u001b[0m         transforms\u001b[39m.\u001b[39mCenterCrop(\u001b[39m224\u001b[39m),\n\u001b[0;32m     13\u001b[0m         transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     14\u001b[0m         transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m     15\u001b[0m     ]),\n\u001b[0;32m     16\u001b[0m }\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitializing Traning and Validation Datasets and Dataloaders...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Create training and validation datasets\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Training: Data aumentation and normalization\n",
    "# Validation: Just normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Traning and Validation Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Choose, reshape, and initialize a model (i.e., AlexNet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[39mreturn\u001b[39;00m model_ft,input_size\n\u001b[0;32m     27\u001b[0m \u001b[39m# Choose the pretrained alexnet model and initialize it proper to our problem  \u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m model_ft, input_size \u001b[39m=\u001b[39m initialize_alexnet(model_name, num_classes, feature_extract)\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(model_ft)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36minitialize_alexnet\u001b[1;34m(model_name, num_classes, feature_extract, use_pretrained)\u001b[0m\n\u001b[0;32m      4\u001b[0m input_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39malexnet\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m   model_ft \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39malexnet(pretrained\u001b[39m=\u001b[39muse_pretrained)\n\u001b[0;32m      9\u001b[0m   \u001b[39m# Freeze the weigths of the network if feauture_extract flag is True \u001b[39;00m\n\u001b[0;32m     10\u001b[0m   \u001b[39mif\u001b[39;00m feature_extract:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "def initialize_alexnet(model_name, num_classes, feature_extract=True, use_pretrained=True):\n",
    "\n",
    "  model_ft = None\n",
    "  input_size = 0\n",
    "\n",
    "  if model_name == \"alexnet\":\n",
    "    model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "\n",
    "    # Freeze the weigths of the network if feauture_extract flag is True \n",
    "    if feature_extract:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Reshape the last layer of the model: FC7(4096 neurons) --> FC8(38 neurons)\n",
    "    # By default, require_grads is True for this new layer\n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    # AlexNet requires the input size to be (224,224)\n",
    "    input_size = 224\n",
    "  else:\n",
    "        print(\"Invalid model name!\")\n",
    "        exit()\n",
    "\n",
    "  return model_ft,input_size\n",
    "\n",
    "# Choose the pretrained alexnet model and initialize it proper to our problem  \n",
    "model_ft, input_size = initialize_alexnet(model_name, num_classes, feature_extract)\n",
    "\n",
    "print(model_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Create the optimizer\n",
    " We create an optimizer that only updates the desired parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "             # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            else: \n",
    "                train_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, train_acc_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss Fuction would be Cross Entropy, as we are doing classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Trans the model and evaluate it\n",
    "model_ft, val_hist, train_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Learning process \n",
    "Let see how the model learns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mAccuracy vs. Number of Training Epochs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mTraining Epochs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.title(\"Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),train_hist,label=\"Training\")\n",
    "plt.plot(range(1,num_epochs+1),val_hist,label=\"Validation\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Save the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/content/drive/My Drive/dl_model_v2_4inf.sav'\n",
    "# model_path = os.getcwd()+'/dl_model_v2_4inf.sav'\n",
    "torch.save(model_ft.state_dict(), model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. Save label/index pairs consistent to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels_path = '/content/drive/My Drive/dl_model_v2_labels.csv'\n",
    "# labels_path = os.getcwd()+'/dl_model_v2_labels.csv'\n",
    "\n",
    "def save_labels(labels_path):\n",
    "    \"\"\"\n",
    "    Save the mapping between labels and index in the model (i.e. exists in ImageFolder) in a csv file\n",
    "    \"\"\"\n",
    "    labels_dict = image_datasets['train'].class_to_idx\n",
    "    labels_df = pd.DataFrame.from_dict(data=labels_dict, orient='index', columns=['idx'])\n",
    "    labels_df['Label'] = labels_df.index # columns=['idx','Label']\n",
    "    labels_df.reset_index(drop=True, inplace=True)\n",
    "    labels_df.to_csv(labels_path, header=None, index=False)\n",
    "\n",
    "save_labels(labels_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a02eb9232b7bee148b638167fb9217ed1c23d9116c4e2381b406e74870b8bc15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
